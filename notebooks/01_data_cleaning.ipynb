{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from pprint import pprint\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/data/sara-salamat/generative-topic-evolution/data/raw\"\n",
    "\n",
    "# list the directories in data_dir and get the name of the directories\n",
    "directories = os.listdir(data_dir)\n",
    "data = {}\n",
    "for directory in directories:\n",
    "    # load json in the directory with unknown name\n",
    "    with open(os.path.join(data_dir, directory, f\"{directory}_notes_with_decisions.json\"), \"r\") as f:\n",
    "        data[directory] = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'original', 'cdate', 'pdate', 'odate', 'mdate', 'tcdate', 'tmdate', 'ddate', 'number', 'content', 'forum', 'referent', 'invitation', 'replyto', 'readers', 'nonreaders', 'signatures', 'writers', 'decision'])\n"
     ]
    }
   ],
   "source": [
    "# print and example of the data\n",
    "pprint(data['neurips2021'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'original', 'cdate', 'pdate', 'odate', 'mdate', 'tcdate', 'tmdate', 'ddate', 'number', 'content', 'forum', 'referent', 'invitation', 'replyto', 'readers', 'nonreaders', 'signatures', 'writers', 'decision'])\n"
     ]
    }
   ],
   "source": [
    "pprint(data['neurips2021'][1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conference neurips2021 has 2768 submissions.\n",
      "Conference neurips2021 has 0 submissions with decisions.\n",
      "Conference neurips2021 has 0.0% submissions with decisions.\n",
      "Conference neurips2022 has 2824 submissions.\n",
      "Conference neurips2022 has 0 submissions with decisions.\n",
      "Conference neurips2022 has 0.0% submissions with decisions.\n",
      "Conference neurips2023 has 3395 submissions.\n",
      "Conference neurips2023 has no decisions.\n",
      "Conference neurips2024 has 4236 submissions.\n",
      "Conference neurips2024 has no decisions.\n"
     ]
    }
   ],
   "source": [
    "# count the number of submissions in each conference\n",
    "# count how many have decisions\n",
    "\n",
    "for conf_name, conf_data in data.items():\n",
    "    print(f\"Conference {conf_name} has {len(conf_data)} submissions.\")\n",
    "    try:\n",
    "        not_none_decisions = [submission for submission in conf_data if submission['decision'] is not None]\n",
    "        print(f\"Conference {conf_name} has {len(not_none_decisions)} submissions with decisions.\")\n",
    "        print(f\"Conference {conf_name} has {len(not_none_decisions) / len(conf_data) * 100}% submissions with decisions.\")\n",
    "    except:\n",
    "        print(f\"Conference {conf_name} has no decisions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Later:\n",
    "\n",
    "- Why data does not have decisions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans input text by removing control characters, unusual unicode symbols,\n",
    "    and invisible or non-ASCII characters, while keeping case, numbers, and punctuation.\n",
    "    \"\"\"\n",
    "    # Normalize Unicode (e.g., decompose accents)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Remove non-printable/control characters\n",
    "    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != \"C\")\n",
    "\n",
    "    # Remove lingering non-ASCII or corrupted characters (e.g., \\uXXXX)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    # Optional: collapse extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conference neurips2021 has 2768 submissions.\n",
      "Schema of data:\n",
      "Keys: dict_keys(['id', 'original', 'cdate', 'pdate', 'odate', 'mdate', 'tcdate', 'tmdate', 'ddate', 'number', 'content', 'forum', 'referent', 'invitation', 'replyto', 'readers', 'nonreaders', 'signatures', 'writers', 'decision'])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Conference neurips2022 has 2824 submissions.\n",
      "Schema of data:\n",
      "Keys: dict_keys(['id', 'original', 'cdate', 'pdate', 'odate', 'mdate', 'tcdate', 'tmdate', 'ddate', 'number', 'content', 'forum', 'referent', 'invitation', 'replyto', 'readers', 'nonreaders', 'signatures', 'writers', 'decision'])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Conference neurips2023 has 3395 submissions.\n",
      "Schema of data:\n",
      "Keys: dict_keys(['id', 'forum', 'content', 'invitations', 'cdate', 'pdate', 'odate', 'mdate', 'signatures', 'writers', 'readers'])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Conference neurips2024 has 4236 submissions.\n",
      "Schema of data:\n",
      "Keys: dict_keys(['id', 'forum', 'content', 'invitations', 'cdate', 'pdate', 'odate', 'mdate', 'signatures', 'writers', 'readers', 'license'])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# fromat to save cleaned processed data:\n",
    "# {\n",
    "#     \"conference_name\": [\n",
    "#         {\n",
    "#             \"id\": \"...\",\n",
    "#             \"TL;DR\": \"...\",\n",
    "#             \"title\": \"...\",\n",
    "#             \"abstract\": \"...\",\n",
    "#             \"authors\": [],\n",
    "#             \"keywords\": [],\n",
    "#             \"venue\": \"...\"\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# Lets print  the schema of data for each conference\n",
    "for conf_name, conf_data in data.items():\n",
    "    print(f\"\\nConference {conf_name} has {len(conf_data)} submissions.\")\n",
    "    print(\"Schema of data:\")\n",
    "    print(\"Keys:\",conf_data[0].keys())\n",
    "    # print(\"Keys of content:\",conf_data[0][\"content\"].keys())\n",
    "    # print(\"Format of title in content:\",conf_data[0][\"content\"][\"title\"])\n",
    "    # print(\"Format of abstract in content:\",conf_data[0][\"content\"][\"abstract\"])\n",
    "    print(\"-\"*100)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_field(content, key):\n",
    "    val = content.get(key) or content.get(key.lower())\n",
    "    return val.get(\"value\") if isinstance(val, dict) else val or \"\"\n",
    "\n",
    "processed_data = {}\n",
    "\n",
    "for conf_name, conf_data in data.items():\n",
    "    cleaned_entries = []\n",
    "    for paper in conf_data:\n",
    "        content = paper.get(\"content\", {})\n",
    "        cleaned = {\n",
    "            \"id\": paper.get(\"id\"),\n",
    "            \"TL;DR\": clean_text(extract_field(content, \"TL;DR\")),\n",
    "            \"title\": clean_text(extract_field(content, \"title\")),\n",
    "            \"abstract\": clean_text(extract_field(content, \"abstract\")),\n",
    "            \"authors\": content.get(\"authors\", []),\n",
    "            \"keywords\": content.get(\"keywords\", []),\n",
    "            \"venue\": content.get(\"venue\")\n",
    "        }\n",
    "        if cleaned[\"title\"] and cleaned[\"abstract\"]:\n",
    "            cleaned_entries.append(cleaned)\n",
    "    processed_data[conf_name] = cleaned_entries\n",
    "with open(\"/mnt/data/sara-salamat/generative-topic-evolution/data/processed/cleaned_data_per_conference.json\", \"w\") as f:\n",
    "    json.dump(processed_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurips2021:\n",
      "  Missing title: 0\n",
      "  Missing abstract: 0\n",
      "  Missing TL;DR: 569\n",
      "--------------------------------------------------\n",
      "neurips2022:\n",
      "  Missing title: 0\n",
      "  Missing abstract: 0\n",
      "  Missing TL;DR: 719\n",
      "--------------------------------------------------\n",
      "neurips2023:\n",
      "  Missing title: 0\n",
      "  Missing abstract: 0\n",
      "  Missing TL;DR: 3395\n",
      "--------------------------------------------------\n",
      "neurips2024:\n",
      "  Missing title: 0\n",
      "  Missing abstract: 0\n",
      "  Missing TL;DR: 4236\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def count_missing_fields_processed(processed_data):\n",
    "    for conf_name, papers in processed_data.items():\n",
    "        missing_title = 0\n",
    "        missing_abstract = 0\n",
    "        missing_tldr = 0\n",
    "\n",
    "        for paper in papers:\n",
    "            if not paper.get(\"title\"):\n",
    "                missing_title += 1\n",
    "            if not paper.get(\"abstract\"):\n",
    "                missing_abstract += 1\n",
    "            if not paper.get(\"TL;DR\"):\n",
    "                missing_tldr += 1\n",
    "\n",
    "        print(f\"{conf_name}:\")\n",
    "        print(f\"  Missing title: {missing_title}\")\n",
    "        print(f\"  Missing abstract: {missing_abstract}\")\n",
    "        print(f\"  Missing TL;DR: {missing_tldr}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run it\n",
    "count_missing_fields_processed(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurips2021 venue categories:\n",
      "  neurips 2021 poster: 2286\n",
      "  neurips 2021 spotlight: 284\n",
      "  neurips 2021 submitted: 136\n",
      "  neurips 2021 oral: 60\n",
      "  <empty venue>: 2\n",
      "--------------------------------------------------\n",
      "neurips2022 venue categories:\n",
      "  neurips 2022 accept: 2671\n",
      "  neurips 2022 submitted: 153\n",
      "  <empty venue>: 0\n",
      "--------------------------------------------------\n",
      "neurips2023 venue categories:\n",
      "  <empty venue>: 3395\n",
      "--------------------------------------------------\n",
      "neurips2024 venue categories:\n",
      "  <empty venue>: 4236\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_venues_with_missing(processed_data):\n",
    "    for conf_name, papers in processed_data.items():\n",
    "        venue_counter = Counter()\n",
    "        missing_count = 0\n",
    "\n",
    "        for paper in papers:\n",
    "            venue = paper.get(\"venue\")\n",
    "            if isinstance(venue, str) and venue.strip():\n",
    "                venue_counter[venue.lower()] += 1\n",
    "            else:\n",
    "                missing_count += 1\n",
    "\n",
    "        print(f\"{conf_name} venue categories:\")\n",
    "        for venue, count in sorted(venue_counter.items(), key=lambda x: -x[1]):\n",
    "            print(f\"  {venue}: {count}\")\n",
    "        print(f\"  <empty venue>: {missing_count}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run it\n",
    "count_venues_with_missing(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
