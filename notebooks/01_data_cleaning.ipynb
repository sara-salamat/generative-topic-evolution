{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from pprint import pprint\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/data/sara-salamat/generative-topic-evolution/data/raw\"\n",
    "\n",
    "# list the directories in data_dir and get the name of the directories\n",
    "directories = os.listdir(data_dir)\n",
    "data = {}\n",
    "for directory in directories:\n",
    "    # load json in the directory with unknown name\n",
    "    with open(os.path.join(data_dir, directory, f\"{directory}_notes_with_decisions.json\"), \"r\") as f:\n",
    "        data[directory] = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cdate': 1621630313567,\n",
      " 'content': {'TL;DR': 'We develop a batch active learning approach that is '\n",
      "                      'effective for very large batch sizes (100K-1M).',\n",
      "             '_bibtex': '@inproceedings{\\n'\n",
      "                        'citovsky2021batch,\\n'\n",
      "                        'title={Batch Active Learning at Scale},\\n'\n",
      "                        'author={Gui Citovsky and Giulia DeSalvo and Claudio '\n",
      "                        'Gentile and Lazaros Karydas and Anand Rajagopalan and '\n",
      "                        'Afshin Rostamizadeh and Sanjiv Kumar},\\n'\n",
      "                        'booktitle={Advances in Neural Information Processing '\n",
      "                        'Systems},\\n'\n",
      "                        'editor={A. Beygelzimer and Y. Dauphin and P. Liang '\n",
      "                        'and J. Wortman Vaughan},\\n'\n",
      "                        'year={2021},\\n'\n",
      "                        'url={https://openreview.net/forum?id=zzdf0CirJM4}\\n'\n",
      "                        '}',\n",
      "             'abstract': 'The ability to train complex and highly effective '\n",
      "                         'models often requires an abundance of training data, '\n",
      "                         'which can easily become a bottleneck in cost, time, '\n",
      "                         'and computational resources. Batch active learning, '\n",
      "                         'which adaptively issues batched queries to a '\n",
      "                         'labeling oracle, is a common approach for addressing '\n",
      "                         'this problem. The practical benefits of batch '\n",
      "                         'sampling come with the downside of less adaptivity '\n",
      "                         'and the risk of sampling redundant examples within a '\n",
      "                         'batch -- a risk that grows with the batch size. In '\n",
      "                         'this work, we analyze an efficient active learning '\n",
      "                         'algorithm, which focuses on the large batch setting. '\n",
      "                         'In particular, we show that our sampling method, '\n",
      "                         'which combines notions of uncertainty and diversity, '\n",
      "                         'easily scales to batch sizes (100K-1M) several '\n",
      "                         'orders of magnitude larger than used in previous '\n",
      "                         'studies and provides significant improvements in '\n",
      "                         'model training efficiency compared to recent '\n",
      "                         'baselines. Finally, we provide an initial '\n",
      "                         'theoretical analysis, proving label complexity '\n",
      "                         'guarantees for a related sampling method, which we '\n",
      "                         'show is approximately equivalent to our sampling '\n",
      "                         'method in specific settings.',\n",
      "             'authorids': ['~Gui_Citovsky1',\n",
      "                           '~Giulia_DeSalvo1',\n",
      "                           '~Claudio_Gentile1',\n",
      "                           '~Lazaros_Karydas2',\n",
      "                           '~Anand_Rajagopalan1',\n",
      "                           '~Afshin_Rostamizadeh1',\n",
      "                           '~Sanjiv_Kumar1'],\n",
      "             'authors': ['Gui Citovsky',\n",
      "                         'Giulia DeSalvo',\n",
      "                         'Claudio Gentile',\n",
      "                         'Lazaros Karydas',\n",
      "                         'Anand Rajagopalan',\n",
      "                         'Afshin Rostamizadeh',\n",
      "                         'Sanjiv Kumar'],\n",
      "             'checklist': '',\n",
      "             'code_of_conduct': 'I certify that all co-authors of this work '\n",
      "                                'have read and commit to adhering to the '\n",
      "                                'NeurIPS Statement on Ethics, Fairness, '\n",
      "                                'Inclusivity, and Code of Conduct.',\n",
      "             'keywords': ['active learning',\n",
      "                          'large scale',\n",
      "                          'batch active learning'],\n",
      "             'paperhash': 'citovsky|batch_active_learning_at_scale',\n",
      "             'pdf': '/pdf/eb75c47ba97fe1a3a11db6d43e727602f83d1c37.pdf',\n",
      "             'submission_history': '',\n",
      "             'submission_history_-_improvements_made': '',\n",
      "             'submission_history_-_venue_and_year': '',\n",
      "             'supplementary_material': '/attachment/6c57c9c423efbffadd75c30e4a5e4889b78a4524.pdf',\n",
      "             'thumbnail': '',\n",
      "             'title': 'Batch Active Learning at Scale',\n",
      "             'venue': 'NeurIPS 2021 Poster',\n",
      "             'venueid': 'NeurIPS.cc/2021/Conference'},\n",
      " 'ddate': None,\n",
      " 'decision': None,\n",
      " 'forum': 'zzdf0CirJM4',\n",
      " 'id': 'zzdf0CirJM4',\n",
      " 'invitation': 'NeurIPS.cc/2021/Conference/-/Blind_Submission',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 11001,\n",
      " 'odate': 1636492716761,\n",
      " 'original': '9et0kjB1YP',\n",
      " 'pdate': 1636492716761,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': None,\n",
      " 'replyto': None,\n",
      " 'signatures': ['NeurIPS.cc/2021/Conference'],\n",
      " 'tcdate': 1621630313567,\n",
      " 'tmdate': 1683307773556,\n",
      " 'writers': ['NeurIPS.cc/2021/Conference']}\n"
     ]
    }
   ],
   "source": [
    "# print and example of the data\n",
    "pprint(data['neurips2021'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'original', 'cdate', 'pdate', 'odate', 'mdate', 'tcdate', 'tmdate', 'ddate', 'number', 'content', 'forum', 'referent', 'invitation', 'replyto', 'readers', 'nonreaders', 'signatures', 'writers', 'decision'])\n"
     ]
    }
   ],
   "source": [
    "pprint(data['neurips2021'][1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conference neurips2021 has 2768 submissions.\n",
      "Conference neurips2021 has 0 submissions with decisions.\n",
      "Conference neurips2021 has 0.0% submissions with decisions.\n",
      "Conference neurips2022 has 2824 submissions.\n",
      "Conference neurips2022 has 0 submissions with decisions.\n",
      "Conference neurips2022 has 0.0% submissions with decisions.\n",
      "Conference neurips2023 has 3395 submissions.\n",
      "Conference neurips2023 has no decisions.\n",
      "Conference neurips2024 has 4236 submissions.\n",
      "Conference neurips2024 has no decisions.\n"
     ]
    }
   ],
   "source": [
    "# count the number of submissions in each conference\n",
    "# count how many have decisions\n",
    "\n",
    "for conf_name, conf_data in data.items():\n",
    "    print(f\"Conference {conf_name} has {len(conf_data)} submissions.\")\n",
    "    try:\n",
    "        not_none_decisions = [submission for submission in conf_data if submission['decision'] is not None]\n",
    "        print(f\"Conference {conf_name} has {len(not_none_decisions)} submissions with decisions.\")\n",
    "        print(f\"Conference {conf_name} has {len(not_none_decisions) / len(conf_data) * 100}% submissions with decisions.\")\n",
    "    except:\n",
    "        print(f\"Conference {conf_name} has no decisions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Later:\n",
    "\n",
    "- Why data does not have decisions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans input text by removing control characters, unusual unicode symbols,\n",
    "    and invisible or non-ASCII characters, while keeping case, numbers, and punctuation.\n",
    "    \"\"\"\n",
    "    # Normalize Unicode (e.g., decompose accents)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Remove non-printable/control characters\n",
    "    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != \"C\")\n",
    "\n",
    "    # Remove lingering non-ASCII or corrupted characters (e.g., \\uXXXX)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    # Optional: collapse extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conference neurips2021 has 2768 submissions.\n",
      "Schema of data:\n",
      "Keys: dict_keys(['id', 'original', 'cdate', 'pdate', 'odate', 'mdate', 'tcdate', 'tmdate', 'ddate', 'number', 'content', 'forum', 'referent', 'invitation', 'replyto', 'readers', 'nonreaders', 'signatures', 'writers', 'decision'])\n",
      "Keys of content: dict_keys(['title', 'authorids', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'supplementary_material', 'submission_history', 'checklist', 'code_of_conduct', 'paperhash', 'thumbnail', 'submission_history_-_venue_and_year', 'submission_history_-_improvements_made', '_bibtex', 'venue', 'venueid'])\n",
      "Format of title in content: Batch Active Learning at Scale\n",
      "Format of abstract in content: The ability to train complex and highly effective models often requires an abundance of training data, which can easily become a bottleneck in cost, time, and computational resources. Batch active learning, which adaptively issues batched queries to a labeling oracle, is a common approach for addressing this problem. The practical benefits of batch sampling come with the downside of less adaptivity and the risk of sampling redundant examples within a batch -- a risk that grows with the batch size. In this work, we analyze an efficient active learning algorithm, which focuses on the large batch setting. In particular, we show that our sampling method, which combines notions of uncertainty and diversity, easily scales to batch sizes (100K-1M) several orders of magnitude larger than used in previous studies and provides significant improvements in model training efficiency compared to recent baselines. Finally, we provide an initial theoretical analysis, proving label complexity guarantees for a related sampling method, which we show is approximately equivalent to our sampling method in specific settings.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Conference neurips2022 has 2824 submissions.\n",
      "Schema of data:\n",
      "Keys: dict_keys(['id', 'original', 'cdate', 'pdate', 'odate', 'mdate', 'tcdate', 'tmdate', 'ddate', 'number', 'content', 'forum', 'referent', 'invitation', 'replyto', 'readers', 'nonreaders', 'signatures', 'writers', 'decision'])\n",
      "Keys of content: dict_keys(['title', 'authorids', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paperhash', 'supplementary_material', 'venue', 'venueid', '_bibtex', 'community_implementations'])\n",
      "Format of title in content: BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework\n",
      "Format of abstract in content: Fusing the camera and LiDAR information has become a de-facto standard for 3D object detection tasks. Current methods rely on point clouds from the LiDAR sensor as queries to leverage the feature from the image space. However, people discovered that this underlying assumption makes the current fusion framework infeasible to produce any prediction when there is a LiDAR malfunction, regardless of minor or major. This fundamentally limits the deployment capability to realistic autonomous driving scenarios. In contrast, we propose a surprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera stream does not depend on the input of LiDAR data, thus addressing the downside of previous methods. We empirically show that our framework surpasses the state-of-the-art methods under the normal training settings. Under the robustness training settings that simulate various LiDAR malfunctions, our framework significantly surpasses the state-of-the-art methods by 15.7% to 28.9% mAP. To the best of our knowledge, we are the first to handle realistic LiDAR malfunction and can be deployed to realistic scenarios without any post-processing procedure. \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Conference neurips2023 has 3395 submissions.\n",
      "Schema of data:\n",
      "Keys: dict_keys(['id', 'forum', 'content', 'invitations', 'cdate', 'pdate', 'odate', 'mdate', 'signatures', 'writers', 'readers'])\n",
      "Keys of content: dict_keys(['title', 'authors', 'authorids', 'keywords', 'abstract', 'venue', 'venueid', 'pdf', 'supplementary_material', '_bibtex', 'TLDR', 'paperhash'])\n",
      "Format of title in content: {'value': 'What is Flagged in Uncertainty Quantification?  Latent Density Models for Uncertainty Categorization'}\n",
      "Format of abstract in content: {'value': 'Uncertainty quantification (UQ) is essential for creating trustworthy machine learning models. Recent years have seen a steep rise in UQ methods that can flag suspicious examples, however, it is often unclear what exactly these methods identify. In this work, we propose a framework for categorizing uncertain examples flagged by UQ methods. We introduce the confusion density matrix---a kernel-based approximation of the misclassification density---and use this to categorize suspicious examples identified by a given uncertainty method into three classes: out-of-distribution (OOD) examples, boundary (Bnd) examples, and examples in regions of high in-distribution misclassification (IDM). Through extensive experiments, we show that our framework provides a new and distinct perspective for assessing differences between uncertainty quantification methods, thereby forming a valuable assessment benchmark.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Conference neurips2024 has 4236 submissions.\n",
      "Schema of data:\n",
      "Keys: dict_keys(['id', 'forum', 'content', 'invitations', 'cdate', 'pdate', 'odate', 'mdate', 'signatures', 'writers', 'readers', 'license'])\n",
      "Keys of content: dict_keys(['title', 'authors', 'authorids', 'keywords', 'TLDR', 'abstract', 'primary_area', 'venue', 'venueid', 'pdf', 'supplementary_material', '_bibtex', 'paperhash'])\n",
      "Format of title in content: {'value': 'Stress-Testing Capability Elicitation With Password-Locked Models'}\n",
      "Format of abstract in content: {'value': 'To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM’s full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task. In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked models, LLMs fine-tuned such that some of their capabilities are deliberately hidden. Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise. Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password. We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. Overall, our findings suggest that fine-tuning is an effective method of eliciting hidden capabilities of current models but may be unreliable when high-quality demonstrations are not available, e.g., as may be the case when models’ (hidden) capabilities exceed those of human demonstrators.'}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# fromat to save cleaned processed data:\n",
    "# {\n",
    "#     \"conference_name\": [\n",
    "#         {\n",
    "#             \"id\": \"...\",\n",
    "#             \"TL;DR\": \"...\",\n",
    "#             \"title\": \"...\",\n",
    "#             \"abstract\": \"...\",\n",
    "#             \"authors\": [],\n",
    "#             \"keywords\": [],\n",
    "#             \"venue\": \"...\"\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# Lets print  the schema of data for each conference\n",
    "for conf_name, conf_data in data.items():\n",
    "    print(f\"\\nConference {conf_name} has {len(conf_data)} submissions.\")\n",
    "    print(\"Schema of data:\")\n",
    "    print(\"Keys:\",conf_data[0].keys())\n",
    "    print(\"Keys of content:\",conf_data[0][\"content\"].keys())\n",
    "    print(\"Format of title in content:\",conf_data[0][\"content\"][\"title\"])\n",
    "    print(\"Format of abstract in content:\",conf_data[0][\"content\"][\"abstract\"])\n",
    "    print(\"-\"*100)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_field(content, key):\n",
    "    val = content.get(key) or content.get(key.lower())\n",
    "    return val.get(\"value\") if isinstance(val, dict) else val or \"\"\n",
    "\n",
    "processed_data = {}\n",
    "\n",
    "for conf_name, conf_data in data.items():\n",
    "    cleaned_entries = []\n",
    "    for paper in conf_data:\n",
    "        content = paper.get(\"content\", {})\n",
    "        cleaned = {\n",
    "            \"id\": paper.get(\"id\"),\n",
    "            \"TL;DR\": clean_text(extract_field(content, \"TL;DR\")),\n",
    "            \"title\": clean_text(extract_field(content, \"title\")),\n",
    "            \"abstract\": clean_text(extract_field(content, \"abstract\")),\n",
    "            \"authors\": content.get(\"authors\", []),\n",
    "            \"keywords\": content.get(\"keywords\", []),\n",
    "            \"venue\": content.get(\"venue\")\n",
    "        }\n",
    "        if cleaned[\"title\"] and cleaned[\"abstract\"]:\n",
    "            cleaned_entries.append(cleaned)\n",
    "    processed_data[conf_name] = cleaned_entries\n",
    "with open(\"/mnt/data/sara-salamat/generative-topic-evolution/data/processed/cleaned_data_per_conference.json\", \"w\") as f:\n",
    "    json.dump(processed_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurips2021:\n",
      "  Missing title: 0\n",
      "  Missing abstract: 0\n",
      "  Missing TL;DR: 569\n",
      "--------------------------------------------------\n",
      "neurips2022:\n",
      "  Missing title: 0\n",
      "  Missing abstract: 0\n",
      "  Missing TL;DR: 719\n",
      "--------------------------------------------------\n",
      "neurips2023:\n",
      "  Missing title: 0\n",
      "  Missing abstract: 0\n",
      "  Missing TL;DR: 3395\n",
      "--------------------------------------------------\n",
      "neurips2024:\n",
      "  Missing title: 0\n",
      "  Missing abstract: 0\n",
      "  Missing TL;DR: 4236\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def count_missing_fields_processed(processed_data):\n",
    "    for conf_name, papers in processed_data.items():\n",
    "        missing_title = 0\n",
    "        missing_abstract = 0\n",
    "        missing_tldr = 0\n",
    "\n",
    "        for paper in papers:\n",
    "            if not paper.get(\"title\"):\n",
    "                missing_title += 1\n",
    "            if not paper.get(\"abstract\"):\n",
    "                missing_abstract += 1\n",
    "            if not paper.get(\"TL;DR\"):\n",
    "                missing_tldr += 1\n",
    "\n",
    "        print(f\"{conf_name}:\")\n",
    "        print(f\"  Missing title: {missing_title}\")\n",
    "        print(f\"  Missing abstract: {missing_abstract}\")\n",
    "        print(f\"  Missing TL;DR: {missing_tldr}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run it\n",
    "count_missing_fields_processed(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurips2021 venue categories:\n",
      "  neurips 2021 poster: 2286\n",
      "  neurips 2021 spotlight: 284\n",
      "  neurips 2021 submitted: 136\n",
      "  neurips 2021 oral: 60\n",
      "  <empty venue>: 2\n",
      "--------------------------------------------------\n",
      "neurips2022 venue categories:\n",
      "  neurips 2022 accept: 2671\n",
      "  neurips 2022 submitted: 153\n",
      "  <empty venue>: 0\n",
      "--------------------------------------------------\n",
      "neurips2023 venue categories:\n",
      "  <empty venue>: 3395\n",
      "--------------------------------------------------\n",
      "neurips2024 venue categories:\n",
      "  <empty venue>: 4236\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_venues_with_missing(processed_data):\n",
    "    for conf_name, papers in processed_data.items():\n",
    "        venue_counter = Counter()\n",
    "        missing_count = 0\n",
    "\n",
    "        for paper in papers:\n",
    "            venue = paper.get(\"venue\")\n",
    "            if isinstance(venue, str) and venue.strip():\n",
    "                venue_counter[venue.lower()] += 1\n",
    "            else:\n",
    "                missing_count += 1\n",
    "\n",
    "        print(f\"{conf_name} venue categories:\")\n",
    "        for venue, count in sorted(venue_counter.items(), key=lambda x: -x[1]):\n",
    "            print(f\"  {venue}: {count}\")\n",
    "        print(f\"  <empty venue>: {missing_count}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run it\n",
    "count_venues_with_missing(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
